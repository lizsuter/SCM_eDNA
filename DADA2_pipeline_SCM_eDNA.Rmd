---
title: "Analysis of Sara's Atlantic fish dataset"
author: "Liz Suter"
date: "Jan 4 2021"
output: html_notebook
editor_options: 
  chunk_output_type: inline
---
[Link](https://htmlpreview.github.io/?https://github.com/lizsuter/SCM_eDNA/blob/main/DADA2_pipeline_SCM_eDNA.nb.html) to notebook  

[Link](https://github.com/lizsuter/SCM_eDNA) to github repo.
<br>


### Install and load packages

```{r}
library(dada2)
library(DECIPHER)
library(phyloseq)
library(taxonomizr)
```


### Get sample names
(Run in bash Terminal)  
```{bash}
cd Raw_data
ls *R1_001.fastq.gz | cut -f 1-2 -d "_" > ../samples
```



### Take a look at the untrimmed reads
```{r}
## import sample names as R variable
samples <- scan("samples", what="character")

# make variable holding the file names of all the forward read fastq files. These are in a subdirectory, so I am also adding the name of the sub directory to the file name
forward_reads <- paste0("Raw_data/", samples, "_L001_R1_001.fastq.gz")
# and one with the reverse
reverse_reads <- paste0("Raw_data/", samples, "_L001_R2_001.fastq.gz")

# And plot using a tool from dada2 (checking only 5 samples for plotting purposes)
plotQualityProfile(forward_reads[1:5])
plotQualityProfile(reverse_reads[1:5])
```

From the above you can see the reads are 150bp and the quality is good, with the R reads being poorer than the F reads. 

### Removing primers using cutadapt- run in Terminal

Primers are universal MiFISh primers from [Miya et al. 2015](https://royalsocietypublishing.org/doi/10.1098/rsos.150088). Amplify a subregion of 12S rRNA that is 163-185 bp long.  

MiFISH-U-F, 5'-3': GTCGGTAAAACTCGTGCCAGC [rev comp: GCTGGCACGAGTTTTACCGAC]  
MiFISH-U-R, 5'-3': CATAGTGGGGTATCTAATCCCAGTTTG [rev comp: CAAACTGGGATTAGATACCCCACTATG]

Cut F primer from F reads with -g option  
Cut rev comp of R primer from F reads with -a option  
Cut R primer from R reads with -G option  
Cut rev comp of F primer from R reads with -A option  

Run cutadapt to remove primers. Use min length of 100bp (bash)

```{bash}
cd ..
mkdir trimmed_fastq
cd Raw_data

# Run in loop

for sample in $(cat ../samples)
do
    echo "On sample: $sample"
cutadapt -g GTCGGTAAAACTCGTGCCAGC \
-a CAAACTGGGATTAGATACCCCACTATG \
-G CATAGTGGGGTATCTAATCCCAGTTTG \
-A GCTGGCACGAGTTTTACCGAC \
-m 100 \
--discard-untrimmed \
-o ../trimmed_fastq/${sample}_L001_R1_001_trimmed.fastq.gz -p ../trimmed_fastq/${sample}_L001_R2_001_trimmed.fastq.gz \
${sample}_L001_R1_001.fastq.gz ${sample}_L001_R2_001.fastq.gz \
>> ../trimmed_fastq/cutadapt_primer_trimming_stats.txt 2>&1
done

```


Check output, how many were filtered out after cutadapt (bash)
```{bash}
paste ../samples <(grep "passing" ../trimmed_fastq/cutadapt_primer_trimming_stats.txt | cut -f3 -d "(" | tr -d ")") <(grep "filtered" ../trimmed_fastq/cutadapt_primer_trimming_stats.txt | cut -f3 -d "(" | tr -d ")")
```

Output: 
```
T1PosCon_S11	89.4%	80.4%
T1S10_S9	81.4%	76.6%
T1S11_S10	97.5%	83.1%
T1S1_S1	97.9%	83.2%
T1S2_S2	68.4%	68.9%
T1S3_S3	85.3%	78.9%
T1S5_S4	91.6%	81.1%
T1S6_S5	85.5%	78.4%
T1S7_S6	83.3%	78.1%
T1S8_S7	98.4%	83.1%
T1S9_S8	94.6%	82.3%
T2PosCon_S21	99.4%	83.5%
T2S10_S19	69.5%	71.7%
T2S11_S20	90.5%	80.7%
T2S1_S12	68.5%	70.6%
T2S2_S13	97.8%	83.1%
T2S3_S14	98.1%	83.2%
T2S4_S15	87.3%	79.8%
T2S5_S16	78.9%	75.9%
T2S6_S17	92.2%	81.3%
T2S9_S18	98.9%	83.5%
T3PosCon_S33	61.0%	67.4%
T3S10_S31	90.7%	80.8%
T3S11_S32	64.6%	69.7%
T3S1_S22	70.2%	72.8%
T3S2_S23	97.5%	82.3%
T3S3_S24	33.2%	45.0%
T3S4_S25	82.3%	77.7%
T3S5_S26	74.4%	74.4%
T3S6_S27	94.1%	82.0%
T3S7_S28	88.1%	79.7%
T3S8_S29	97.0%	82.7%
T3S9_S30	95.8%	82.6%
T4S10_S43	89.3%	80.2%
T4S11_S44	90.3%	80.2%
T4S1_S34	97.4%	83.0%
T4S2_S35	77.8%	76.2%
T4S3_S36	96.4%	82.6%
T4S4_S37	79.2%	74.6%
T4S5_S38	93.5%	79.5%
T4S6_S39	88.9%	79.8%
T4S7_S40	92.9%	81.6%
T4S8_S41	91.1%	81.0%
T4S9_S42	77.7%	76.2%
T5S10_S54	92.7%	81.3%
T5S11_S55	84.4%	78.4%
T5S1_S45	95.3%	82.1%
T5S2_S46	96.3%	82.2%
T5S3_S47	97.8%	83.0%
T5S4_S48	96.4%	82.4%
T5S5_S49	83.8%	78.2%
T5S6_S50	89.2%	79.8%
T5S7_S51	78.3%	75.1%
T5S8_S52	96.3%	82.3%
T5S9_S53	95.3%	82.2%
```

So it retained ~60-99% of reads. Looking at the `cutadapt_primer_trimming_stats.txt` output file, you can see the F and R primers were trimmed in almost all cases and the rev comp of each primer was trimmed in many cases too.


# DADA2


Go back to R console and take a look at the trimmed reads.

```{r}
forward_reads_trimmed <- paste0("trimmed_fastq/", samples, "_L001_R1_001_trimmed.fastq.gz")
reverse_reads_trimmed <- paste0("trimmed_fastq/", samples, "_L001_R2_001_trimmed.fastq.gz")

# And plot 
plotQualityProfile(forward_reads_trimmed[1:5])
plotQualityProfile(reverse_reads_trimmed[1:5])
```

Comparing the above to the pre-trimmed reads, they look very similar because only very few had primers. 

### Quality Trimming
Make a directory for filtered reads (bash)
```{bash}
cd ..
mkdir filtered_fastq
```



Make variables containing the file names for the new filtered forward and reverse reads that we will make
```{r}
filtered_forward_reads <- paste0("filtered_fastq/",samples, "_L001_R1_001_filtered.fastq.gz")
filtered_reverse_reads <- paste0("filtered_fastq/",samples, "_L001_R2_001_filtered.fastq.gz")
```



Based on how the quality plots look, determine how much to cut from each side. Trim the F reads at 130 Trim the R reads to 120 Also I want to run this step to trim out the low-quality individual reads (set maxEE to 1 for both F and R reads). The rm.phix = TRUE option removes any leftover  PhiX genomic DNA (that gets added as a standard during sequencing). Pick a min length ~shorter than the min trimmed length (in this case 100 for R reads). I also set truncQ to truncate any read that has a quality score less than 2. Multithreading for this function does not work well (even according to documentation) so needed to skip that. Takes awhile  to run.
```{r}
filtered_out <- filterAndTrim(forward_reads_trimmed, filtered_forward_reads,
                reverse_reads_trimmed, filtered_reverse_reads, maxEE=c(2,2),
                rm.phix=TRUE, minLen=100, truncLen=c(130,120), truncQ = 2, maxN=0)
```



Check out the quality profiles again.
```{r}
filtered_out

plotQualityProfile(filtered_forward_reads[1:5])
plotQualityProfile(filtered_reverse_reads[1:5])
```


These look better. Many were retained before and after quality trimming:


Save workspace up to this point.
```{r}
save.image(file = "upto_filterfastq.RData")
```



Run if you come back and need to reload dataset
```{r}
load("upto_filterfastq.RData")
```


### Error profiling
Next, DADA2 tries to learn the error signature of our dataset. This step takes a while
```{r}
err_forward_reads <- learnErrors(filtered_forward_reads, multithread=TRUE)
err_reverse_reads <- learnErrors(filtered_reverse_reads, multithread=TRUE)
```



Plot the error profiles
```{r}
plotErrors(err_forward_reads, nominalQ=TRUE)
plotErrors(err_reverse_reads, nominalQ=TRUE)
```

The creators of DADA2 describe this [here](https://benjjneb.github.io/dada2/tutorial.html#learn-the-error-rates). The profiles are the error rates for each possible transition in the read (A->C, A->G, etc). Generally in the above plots, you want to see that the black dots (observed error rates for each quality score) match well with the black lines (the estimated error rate). The red line is what is expected based on the quality score.

Backup again since this step above takes awhile  
Save workspace up to this point.
```{r}
save.image(file = "upto_errorprofile.RData")
```



### Inferring ASVs
Use the dada command to infer ASVs. This step also takes awhile

Stoeckle et al. use the loess error model for error estimation, which is the default so I didn't set that here. They also set selfConsist = true so that "the error-model was independently built for each sample." They also turn pooling off (the default) and said that they "build an error model using a subset of the total reads from a sequencing run and provide this model to DADA2." However, in their code, they set the error model as `err=inflateErr(tperr1,3)` which uses an error matrix, `tperr1`, from a mock community. This is used as an example in the dada2 documentation but I don't think it should be used for real samples. Really they should have generated the model from their own data (as above). So I am keeping my own modifications here and not theirs. This error model is generated from the samples themselves, with pseudopooling across samples. According to the developers: "pooling information across samples can increase sensitivity to sequence variants that may be present at very low frequencies in multiple samples." The dada2 package offers two types of pooling, complete sample pooling (which is computationally expensive) and pseudo-pooling, which gives the benefit of pooling but is not as intensive. This is further described at the developer's [site](https://benjjneb.github.io/dada2/pseudo.html#pseudo-pooling). 
```{r}
dada_forward <- dada(filtered_forward_reads, err=err_forward_reads, pool="pseudo", multithread=TRUE) 
dada_reverse <- dada(filtered_reverse_reads, err=err_reverse_reads, pool="pseudo", multithread=TRUE)
```



Backup again since this step above takes awhile  

Save workspace up to this point.
```{r}
save.image(file = "upto_inferasv.RData")
```



### Merge inferred reads 
Dada2 will merge reads wherever the overlap is identical between the F and R reads. I trimmed the F reads to 130 and R reads to 120 (150). The full amplicon size (based on primers) should be 163-185. Being conservative, use a length of 185. This means the F read is from position 1 to 130 and the R read is from position 185 to 65, leaving a region of overlap between position 65 and 130, or 65 total bp.  
Since this is an estimate, leave a little wiggle room and set the minimum overlap to 30bp. Also set trimOverhang to true, which makes sure that a read doesn't go past its opposite primer (which probably wouldn't happpen any way due to trimming).
```{r}
merged_amplicons <- mergePairs(dada_forward, filtered_forward_reads, dada_reverse, filtered_reverse_reads, trimOverhang=TRUE, minOverlap=30, verbose = TRUE)
```



```{r}
names(merged_amplicons)
# Initially these names have the full name with `fastq.gz` in the name. Change to just sample name
names(merged_amplicons) <- samples

# Check some other things
length(merged_amplicons) # one for each of our samples
class(merged_amplicons$T1PosCon_S11) # each element of the list is a dataframe that can be accessed and manipulated like any ordinary dataframe
names(merged_amplicons$T1PosCon_S11) # the names() function on a dataframe gives you the column names
# "sequence"  "abundance" "forward"   "reverse"   "nmatch"    "nmismatch" "nindel"    "prefer"    "accept"
```



Back up again
```{r}
save.image(file = "upto_merge.RData")
```


### Creating a sequence table
```{r}
seqtab <- makeSequenceTable(merged_amplicons)
class(seqtab) # matrix
dim(seqtab) # 55 samples, 3512 unique ASVs
```

### Removing chimeras
```{r}
seqtab.nochim <- removeBimeraDenovo(seqtab, verbose=TRUE) 

# Identified 3121 bimeras out of 3512 input sequences.

# though we got a lot of unique sequences, we don't know if they held a lot in terms of abundance, this is one quick way to look at that
sum(seqtab.nochim)/sum(seqtab) 
# 0.9457455
# good, we barely lost any in terms of abundance. That means the chimeras were very low abundance ASVs

dim(seqtab.nochim) # 55 samples, 391 unique ASVs remain

```

Backup again since this step above takes awhile
Save workspace up to this point.
```{r}
save.image(file = "upto_chimera.RData")
```


### Summary of read counts through the pipeline
```{r}
# set function from Happy Belly
getN <- function(x) sum(getUniques(x))

# making a little table
summary_tab <- data.frame(row.names=samples, dada2_input=filtered_out[,1],
                          filtered=filtered_out[,2], dada_f=sapply(dada_forward, getN),
                          dada_r=sapply(dada_reverse, getN), merged=sapply(merged_amplicons, getN),
                          nonchim=rowSums(seqtab.nochim),
                          final_perc_reads_retained=round(rowSums(seqtab.nochim)/filtered_out[,1]*100, 1))

summary_tab
```

In the end, we retained abot 80-95% of input reads after the filtering steps. (Except 3 samples that didn't merge at all.)


Next, follow some code from [Stoeckle et al.](https://journals.plos.org/plosone/article?id=10.1371/journal.pone.0175186#sec014) for making `writeFasta` function and exporting the "OTU" table and fasta file.  
(Note this is not at "OTU" table because these are ASVs)

```{r}
# Construct  sequence table
table(nchar(colnames(seqtab.nochim)))
length(unique(substr(colnames(seqtab.nochim), 1, 100)))
dim(seqtab.nochim)

# Make "otu_table"
seqs <- colnames(seqtab.nochim)
otab <- otu_table(seqtab.nochim, taxa_are_rows=FALSE)
colnames(otab) <- paste0("Seq_", seq(ncol(otab)))
#Write fastas to test file
writeFasta <- function(seqs, output) {

  seqsout <- mapply( function(idx, sequence) paste0(">Seq_",idx,"\n",sequence,"\n"),
                     seq(length(seqs)),
                     seqs)
  write(paste0(seqsout), file = output, sep = "")

}

seqs_for_blast <- DNAStringSet(seqs)
names(seqs_for_blast) <-  sapply(seq(length(seqs)),function(x) {paste0("Seq_",x)})

# Write the fasta sequences and the OTU table
writeFasta(seqs, "results/tax_sequences.fasta")
write.table(otab,  file="results/otutable.csv", col.names = NA)
```




## Annotation

I am using blastn function in terminal, which can be installed with the blast package from NCBI using conda `conda install blast`. I already had it installed but was having trouble bc v2.9 does not work on remote host (according to [this](https://www.biostars.org/p/400841/)). So I downgraded my blast to v2.6 with `conda install -c bioconda/label/cf201901 blast`


#### Run blastn on remote NCBI server
Use -max_target_seqs of 1 for now. Only puts top hit in the file
```{bash}
blastn -query results/tax_sequences.fasta \
-db nt \
-out results/tax_sequences_blast.txt \
-remote \
-max_target_seqs 1 \
-outfmt "6 qseqid sseqid pident length mismatch evalue bitscore staxids stitle" 
```

The headings of the table are the following parameters from NCBI:  
- seqid:	 query (e.g., unknown gene) sequence id  
- sseqid:	 subject (e.g., reference genome) sequence id  
- pident:	 percentage of identical matches  
- length:	 alignment length (sequence overlap)  
- mismatch:	 number of mismatches  
- evalue:	 expect value  
- bitscore:	 bit score  
- staxids:   Subject Taxonomy ID(s), separated by a ';'  
- stitle:      Subject Title  



#### Link Accession Numbers with Taxonomy from NCBI
I found an R package for this called [Taxonomizr](https://github.com/sherrillmix/taxonomizr).


Download all tax assignments in NCBI and make SQLite database (this takes several hours). Store this file in a shared place instead of downloading it each time (because it's huge)

```{r}
prepareDatabase('databases/accessionTaxa.sql')
```



Read blast results from  table into R and give it headers
```{r}
blastResults<-read.table('results/tax_sequences_blast.txt', header=FALSE, stringsAsFactors=FALSE,fill=TRUE)
colnames(blastResults) <- c("ASV_ID", "ref_seq_ID", "PID", "alnmt_len", "mismatch", "eval", "bscore", "RefSeq_Tax_ID", "Ref_Seq_title")
blastResults[1:10,]
```



and grab accesion numbers
```{r}
#grab the 4th |-separated field from the reference name in the second column
accessions<-sapply(strsplit(blastResults[,2],'\\|'),'[',4)
accessions[1:10]
```



Get taxonomy ID for each accesion number
```{r}
taxaId<-accessionToTaxa(accessions,"databases/accessionTaxa.sql")
taxaId[1:10]
```



And get taxonomy for those tax IDs
```{r}
taxonomy_table <- getTaxonomy(taxaId,'databases/accessionTaxa.sql')

taxonomy_table <- cbind(taxonomy_table, rownames(taxonomy_table))
colnames(taxonomy_table)[dim(taxonomy_table)[2]] <- "RefSeq_Tax_ID"
rownames(taxonomy_table) <- NULL

taxonomy_table <- as.data.frame(taxonomy_table)
taxonomy_table[1:10,]
```



Append taxonomy table to blast results table and export as one file
```{r}
tax_sequences_blast_taxonomy <- cbind(blastResults, taxonomy_table)
write.table(tax_sequences_blast_taxonomy, file = "results/tax_sequences_blast_taxonomy.csv", sep = ",", col.names=NA)

```






